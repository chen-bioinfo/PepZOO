{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable, from_csv\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from Bio import SeqIO\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 氨基酸集合（字典）\n",
    "amino_acid_set = { \"A\": 1, \"C\": 2, \"B\": 3, \"E\": 4, \"D\": 5, \"G\": 6, \n",
    "\t\t\t\t\"F\": 7, \"I\": 8, \"H\": 9, \"K\": 10, \"M\": 11, \"L\": 12, \n",
    "\t\t\t\t\"O\": 13, \"N\": 14, \"Q\": 15, \"P\": 16, \"S\": 17, \"R\": 18, \n",
    "\t\t\t\t\"U\": 19, \"T\": 20, \"W\": 21, \n",
    "\t\t\t\t\"V\": 22, \"Y\": 23, \"X\": 24, \n",
    "\t\t\t\t\"Z\": 25 } # consider non-standard residues\n",
    "# 氨基酸数目\n",
    "amino_acid_num = 25\n",
    "# 二级结构类别集合（字典）\n",
    "ss_set = {\"H\": 1, \"C\": 2, \"E\": 3} # revise order, not necessary if training your own model\n",
    "# 二级结构类别数目\n",
    "ss_number = 3\n",
    "# 氨基酸类别集合（字典）\n",
    "physicochemical_set={'A': 1, 'C': 3, 'B': 7, 'E': 5, 'D': 5, 'G': 2, 'F': 1, \n",
    "\t\t\t'I': 1, 'H': 6, 'K': 6, 'M': 1, 'L': 1, 'O': 7, 'N': 4, \n",
    "\t\t\t'Q': 4, 'P': 1, 'S': 4, 'R': 6, 'U': 7, 'T': 4, 'W': 2, \n",
    "\t\t\t'V': 1, 'Y': 4, 'X': 7, 'Z': 7}\n",
    "# 残基列表\n",
    "residue_list = list(amino_acid_set.keys())\n",
    "# 二级结构列表\n",
    "ss_list = list(ss_set.keys())\n",
    "\n",
    "\n",
    "new_key_list = []\n",
    "for i in residue_list:\n",
    "    for j in ss_list:\n",
    "        str_1 = str(i)+str(j)\n",
    "        new_key_list.append(str_1)\n",
    "\n",
    "new_value_list = [x+1 for x in list(range(amino_acid_num*ss_number))]\n",
    "\n",
    "seq_ss_dict = dict(zip(new_key_list,new_value_list))\n",
    "seq_ss_number = amino_acid_num*ss_number #75\n",
    "\n",
    "\n",
    "\n",
    "def label_sequence(line, pad_prot_len, res_ind):\n",
    "\tX = np.zeros(pad_prot_len)\n",
    "\n",
    "\tfor i, res in enumerate(line[:pad_prot_len]):\n",
    "\t\tX[i] = res_ind[res]\n",
    "\n",
    "\treturn X\n",
    "\n",
    "def label_seq_ss(line, pad_prot_len, res_ind):\n",
    "\tline = line.strip().split(',')\n",
    "\tX = np.zeros(pad_prot_len)\n",
    "\tfor i ,res in enumerate(line[:pad_prot_len]):\n",
    "\t\tX[i] = res_ind[res]\n",
    "\treturn X\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "\treturn 1 / (1 + math.exp(-x))\n",
    "\n",
    "sigmoid_array=np.vectorize(sigmoid)\n",
    "\n",
    "def padding_sigmoid_pssm(x,N):\n",
    "\tx = sigmoid_array(x)\n",
    "\tpadding_array = np.zeros([N,x.shape[1]])\n",
    "\tif x.shape[0]>=N: # sequence is longer than N\n",
    "\t\tpadding_array[:N,:x.shape[1]] = x[:N,:]\n",
    "\telse:\n",
    "\t\tpadding_array[:x.shape[0],:x.shape[1]] = x\n",
    "\treturn padding_array\n",
    "\n",
    "def padding_intrinsic_disorder(x,N):\n",
    "\tpadding_array = np.zeros([N,x.shape[1]])\n",
    "\tif x.shape[0]>=N: # sequence is longer than N\n",
    "\t\tpadding_array[:N,:x.shape[1]] = x[:N,:]\n",
    "\telse:\n",
    "\t\tpadding_array[:x.shape[0],:x.shape[1]] = x\n",
    "\treturn padding_array\n",
    "\n",
    "\n",
    "def cls_scores(label, pred):\n",
    "\tlabel = label.reshape(-1)\n",
    "\tpred = pred.reshape(-1)\n",
    "\t# r2_score, mean_squred_error are ignored\n",
    "\treturn roc_auc_score(label, pred), average_precision_score(label, pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalMaxPool1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalMaxPool1d,self).__init__()\n",
    "    def forward(self,x):\n",
    "        output, _ = torch.max(x,1)\n",
    "        return output\n",
    "\n",
    "class ConvNN(nn.Module):\n",
    "    def __init__(self,in_dim,c_dim,kernel_size):\n",
    "        super(ConvNN,self).__init__()\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_dim, out_channels= c_dim, kernel_size=kernel_size,padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=c_dim, out_channels= c_dim*2, kernel_size=kernel_size,padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=c_dim*2, out_channels= c_dim*3, kernel_size=kernel_size,padding='same'),\n",
    "            nn.ReLU(),\n",
    "            #GlobalMaxPool1d() # 192\n",
    "            )\n",
    "    def forward(self,x):\n",
    "        x = self.convs(x)\n",
    "        return x\n",
    "\n",
    "class Self_Attention(nn.Module):\n",
    "    # input : batch_size * seq_len * input_dim\n",
    "    # q : batch_size * input_dim * dim_k\n",
    "    # k : batch_size * input_dim * dim_k\n",
    "    # v : batch_size * input_dim * dim_v\n",
    "    def __init__(self,input_dim,dim_k,dim_v):\n",
    "        super(Self_Attention,self).__init__()\n",
    "        self.q = nn.Linear(input_dim,dim_k)\n",
    "        self.k = nn.Linear(input_dim,dim_k)\n",
    "        self.v = nn.Linear(input_dim,dim_v)\n",
    "        self._norm_fact = 1 / math.sqrt(dim_k)\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        Q = self.q(x) # Q: batch_size * seq_len * dim_k\n",
    "        K = self.k(x) # K: batch_size * seq_len * dim_k\n",
    "        V = self.v(x) # V: batch_size * seq_len * dim_v\n",
    "         \n",
    "        atten = nn.Softmax(dim=-1)(torch.bmm(Q,K.permute(0,2,1))) * self._norm_fact # Q * K.T() # batch_size * seq_len * seq_len\n",
    "        \n",
    "        output = torch.bmm(atten,V) # Q * K.T() * V # batch_size * seq_len * dim_v\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class CAMP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CAMP,self).__init__()\n",
    "        #self.config = config\n",
    "        self.embed_seq = nn.Embedding(65+1, 128) # padding_idx=0, vocab_size = 65/25, embedding_size=128\n",
    "        self.embed_ss = nn.Embedding(75+1,128)\n",
    "        self.embed_two = nn.Embedding(7+1,128)\n",
    "        self.pep_convs = ConvNN(512,64,7)\n",
    "        self.prot_convs = ConvNN(512,64,8)\n",
    "        self.pep_fc = nn.Linear(3,128)    \n",
    "        self.prot_fc = nn.Linear(23,128)\n",
    "        self.global_max_pooling = GlobalMaxPool1d()\n",
    "        #self.dnns = DNN(config.in_dim,config.d_dim1,config.d_dim2,config.dropout)\n",
    "        self.dnns = nn.Sequential(\n",
    "            nn.Linear(640,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(1024,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(1024,512))\n",
    "        \n",
    "        self.att = Self_Attention(128,128,128)\n",
    "        #c_dim\n",
    "        self.output = nn.Linear(512,1)\n",
    "\n",
    "    #@torchsnooper.snoop()\n",
    "    def forward(self, x_pep,x_prot,x_pep_ss,x_prot_ss,x_pep_2,x_prot_2,x_pep_dense,x_prot_dense):\n",
    "\n",
    "        pep_seq_emb = self.embed_seq(x_pep.long())#.type(torch.LongTensor))\n",
    "        prot_seq_emb = self.embed_seq(x_prot.long())#.type(torch.LongTensor))\n",
    "        pep_ss_emb = self.embed_ss(x_pep_ss.long())#type(torch.LongTensor))\n",
    "        prot_ss_emb = self.embed_ss(x_prot_ss.long())\n",
    "        pep_2_emb = self.embed_two(x_pep_2.long())\n",
    "        prot_2_emb = self.embed_two(x_prot_2.long())\n",
    "        pep_dense = self.pep_fc(x_pep_dense.float())\n",
    "        prot_dense = self.prot_fc(x_prot_dense.float())\n",
    "        \n",
    "\n",
    "        encode_peptide = torch.cat([pep_seq_emb, pep_ss_emb, pep_2_emb, pep_dense],dim=-1)\n",
    "        encode_protein = torch.cat([prot_seq_emb, prot_ss_emb, prot_2_emb, prot_dense],dim=-1)\n",
    "\n",
    "        encode_peptide = encode_peptide.permute(0,2,1)\n",
    "        encode_protein = encode_protein.permute(0,2,1)\n",
    "\n",
    "        encode_peptide = self.pep_convs(encode_peptide)\n",
    "        encode_peptide = encode_peptide.permute(0,2,1)\n",
    "        encode_peptide_global = self.global_max_pooling(encode_peptide)\n",
    "\n",
    "        encode_protein = self.prot_convs(encode_protein)\n",
    "        encode_protein = encode_protein.permute(0,2,1)\n",
    "        encode_protein_global = self.global_max_pooling(encode_protein)\n",
    "        \n",
    "        # self-attention\n",
    "        pep_seq_att = self.embed_seq(x_pep.long())\n",
    "        peptide_att = self.att(pep_seq_att)\n",
    "        peptide_att = self.global_max_pooling(peptide_att)\n",
    "        \n",
    "        prot_seq_att = self.embed_seq(x_prot.long())\n",
    "        protein_att = self.att(prot_seq_att)\n",
    "        protein_att = self.global_max_pooling(protein_att)\n",
    "\n",
    "        encode_interaction = torch.cat([encode_peptide_global,encode_protein_global,peptide_att,protein_att],axis=-1)\n",
    "        encode_interaction = self.dnns(encode_interaction)\n",
    "        predictions = torch.sigmoid(self.output(encode_interaction))\n",
    "\n",
    "        return predictions.squeeze(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    ckpt = torch.load(filepath, map_location=torch.device('cpu'))\n",
    "    model = ckpt['model']\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad=False\n",
    "    model.eval()\n",
    "    return model\n",
    "    \n",
    "device = torch.device('cuda:1')\n",
    "camp = load_checkpoint('/geniusland/home/liuxianliang1/code/PeptideOpt_4090/amp/inference/CAMP/CAMP_pytorch/model_full_ckpts_4.pkl')\n",
    "camp = camp.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv('data/Antimicrobials.csv')\n",
    "mask = data['sequence'].str.len() <= 25\n",
    "peptides = data.loc[mask]['sequence'].tolist()\n",
    "peps_len = len(peptides)\n",
    "# cxcr4\n",
    "protein = 'MEGISIYTSDNYTEEMGSGDYDSMKEPCFREENANFNKIFLPTIYSIIFLTGIVGNGLVI\\\n",
    "LVMGYQKKLRSMTDKYRLHLSVADLLFVITLPFWAVDAVANWYFGNFLCKAVHVIYTVNL\\\n",
    "YSSVLILAFISLDRYLAIVHATNSQRPRKLLAEKVVYVGVWIPALLLTIPDFIFANVSEA\\\n",
    "DDRYICDRFYPNDLWVVVFQFQHIMVGLILPGIVILSCYCIIISKLSHSKGHQKRKALKT\\\n",
    "TVILILAFFACWLPYYIGISIDSFILLEIIKQGCEFENTVHKWISITEALAFFHCCLNPI\\\n",
    "LYAFLGAKFKTSAQHALTSVSRGSSLKILSKGKRGGHSSVSTESESSSFHSS'\n",
    "\n",
    "# acthr\n",
    "# protein = 'MKHIINSYENINNTARNNSDCPRVVLPEEIFFTISIVGVLENLIVLLAVFKNKNLQAPMY\\\n",
    "# FFICSLAISDMLGSLYKILENILIILRNMGYLKPRGSFETTADDIIDSLFVLSLLGSIFS\\\n",
    "# LSVIAADRYITIFHALRYHSIVTMRRTVVVLTVIWTFCTGTGITMVIFSHHVPTVITFTS\\\n",
    "# LFPLMLVFILCLYVHMFLLARSHTRKISTLPRANMKGAITLTILLGVFIFCWAPFVLHVL\\\n",
    "# LMTFCPSNPYCACYMSLFQVNGMLIMCNAVIDPFIYAFRSPELRDAFKKMIFCSRYW'\n",
    "\n",
    "with open('features/amps.fasta', 'w') as wf:\n",
    "    for i in range(peps_len):\n",
    "        f = open(f'features/{i+1}.fasta','w')\n",
    "        f.write('>peptide')\n",
    "        f.write('\\n')\n",
    "        f.write(peptides[i])\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "\n",
    "        wf.write(f'>{i+1}')\n",
    "        wf.write('\\n')\n",
    "        wf.write(peptides[i])\n",
    "        wf.write('\\n')\n",
    "\n",
    "f = open('features/target_protein.fasta','w')\n",
    "f.write('>target_protein')\n",
    "f.write('\\n')\n",
    "f.write(protein)\n",
    "f.write('\\n')\n",
    "f.close()               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取序列二级结构信息\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###################################\n",
      "#                                 #\n",
      "#  SCRATCH-1D release 1.2 (2018)  #\n",
      "#                                 #\n",
      "###################################\n",
      "\n",
      "[SCRATCH-1D_predictions.pl] 10 protein sequence(s) found\n",
      "[SCRATCH-1D_predictions.pl] generating sequence profiles...\n",
      "[SCRATCH-1D_predictions.pl] running SCRATCH-1D predictors...\n",
      "[SCRATCH-1D_predictions.pl] running homology analysis...\n",
      "[SCRATCH-1D_predictions.pl] writing SSpro predictions...\n",
      "[SCRATCH-1D_predictions.pl] writing SSpro8 predictions...\n",
      "[SCRATCH-1D_predictions.pl] writing ACCpro predictions...\n",
      "[SCRATCH-1D_predictions.pl] writing ACCpro20 predictions...\n",
      "[SCRATCH-1D_predictions.pl] job successfully completed!\n",
      "\n",
      "\n",
      "###################################\n",
      "#                                 #\n",
      "#  SCRATCH-1D release 1.2 (2018)  #\n",
      "#                                 #\n",
      "###################################\n",
      "\n",
      "[SCRATCH-1D_predictions.pl] 1 protein sequence(s) found\n",
      "[SCRATCH-1D_predictions.pl] generating sequence profiles...\n",
      "[SCRATCH-1D_predictions.pl] running SCRATCH-1D predictors...\n",
      "[SCRATCH-1D_predictions.pl] running homology analysis...\n",
      "[SCRATCH-1D_predictions.pl] writing SSpro predictions...\n",
      "[SCRATCH-1D_predictions.pl] writing SSpro8 predictions...\n",
      "[SCRATCH-1D_predictions.pl] writing ACCpro predictions...\n",
      "[SCRATCH-1D_predictions.pl] writing ACCpro20 predictions...\n",
      "[SCRATCH-1D_predictions.pl] job successfully completed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('features/peptides.out.ss'):\n",
    "    os.system(\"/geniusland/home/liuxianliang1/code/PeptideOpt_4090/SCRATCH-1D_1.2/bin/run_SCRATCH-1D_predictors.sh\\\n",
    "        features/amps.fasta features/peptides.out 32\")\n",
    "if not os.path.exists('features/protein.out.ss'):\n",
    "    os.system(\"/geniusland/home/liuxianliang1/code/PeptideOpt_4090/SCRATCH-1D_1.2/bin/run_SCRATCH-1D_predictors.sh\\\n",
    "        features/target_protein.fasta features/protein.out 32\")\n",
    "# 保存序列以及对应的二级结构信息\n",
    "wf = open('features/example.tsv','w')\n",
    "tsv_w = csv.writer(wf, delimiter='\\t')\n",
    "tsv_w.writerow(['prot_seq', 'pep_seq', 'pep_concat_seq', 'prot_concat_seq'])\n",
    "\n",
    "with open('features/peptides.out.ss', 'r') as f:\n",
    "    peps_data = f.readlines()\n",
    "for i in range(len(peps_data)):\n",
    "    peps_data[i] = peps_data[i].strip('\\n')\n",
    "with open('features/protein.out.ss', 'r') as f:\n",
    "    prot_data = f.readlines()\n",
    "for i in range(len(prot_data)):\n",
    "    prot_data[i] = prot_data[i].strip('\\n')\n",
    "\n",
    "# 目标蛋白\n",
    "prot_seq = protein\n",
    "# 目标蛋白二级结构信息\n",
    "prot_concat_seq = \"\"\n",
    "for j in range(len(prot_seq)):\n",
    "    if j < len(prot_seq) - 1:\n",
    "        prot_concat_seq = prot_concat_seq + prot_seq[j] + prot_data[1][j] + ','\n",
    "    else:\n",
    "        prot_concat_seq = prot_concat_seq + prot_seq[j] + prot_data[1][j]\n",
    "# 多肽二级结构信息\n",
    "pep_concat_seq = \"\"\n",
    "filename = 'features/amps.fasta'\n",
    "iterator = SeqIO.parse(filename,'fasta')\n",
    "seqs = []\n",
    "for record in iter(iterator):\n",
    "    seqs.append(record)\n",
    "\n",
    "for i in range(len(seqs)):\n",
    "    for j in range(len(seqs[i].seq)):\n",
    "        if j < len(seqs[i].seq) - 1:\n",
    "            pep_concat_seq = pep_concat_seq + seqs[i].seq[j] + peps_data[2*i+1][j] + ','\n",
    "        else:\n",
    "            pep_concat_seq = pep_concat_seq + seqs[i].seq[j] + peps_data[2*i+1][j]\n",
    "    tsv_w.writerow([prot_seq, seqs[i].seq, pep_concat_seq, prot_concat_seq])\n",
    "    pep_concat_seq = \"\"\n",
    "wf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算目标蛋白的pssm矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_pssm_dict = {}   \n",
    "# 调用命令行计算多序列比对结果(pssm)\n",
    "if not os.path.exists('features/target_protein.pssm'):\n",
    "    os.system(\"psiblast -query /geniusland/home/liuxianliang1/code/PeptideOpt_4090/features/target_protein.fasta -db /geniusland/dataset/uniprot/uniref90/uniref90.fasta -num_iterations 3 -out_ascii_pssm features/target_protein.pssm\")\n",
    "# 解析结果\n",
    "with open('/geniusland/home/liuxianliang1/code/PeptideOpt_4090/features/target_protein.pssm','r') as rf:\n",
    "    data = rf.readlines()\n",
    "tmp = np.zeros((len(protein),20))\n",
    "for i in range(3, 3+len(protein)):\n",
    "    char = data[i].split(' ')\n",
    "    count = 0\n",
    "    for j in range(6, len(char)):\n",
    "        if char[j] != char[0]:\n",
    "            tmp[i-3][count] = float(char[j])\n",
    "            count += 1\n",
    "        if count >= 20:\n",
    "            break\n",
    "prot_pssm_dict[protein] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算内在紊乱度信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_intrinsic_dict = {}\n",
    "pep_intrinsic_dict = {}\n",
    "# 多肽序列\n",
    "for i in range(peps_len):\n",
    "    result = os.popen(f\"python3 /geniusland/home/liuxianliang1/code/PeptideOpt_4090/amp/iupred2a/iupred2a.py -a /geniusland/home/liuxianliang1/code/PeptideOpt_4090/features/{i+1}.fasta long\")\n",
    "    res = result.read()\n",
    "    long_val = res.splitlines()[0].split(',')\n",
    "    long_list = np.zeros((len(long_val), 1))\n",
    "    long_list[0][0] = float(long_val[0].split('[')[1])\n",
    "    for j in range(1, len(long_val)-1):\n",
    "        long_list[j][0] = float(long_val[j])\n",
    "    long_list[j][0] = float(long_val[j].split(']')[0])\n",
    "\n",
    "    result = os.popen(f\"python3 /geniusland/home/liuxianliang1/code/PeptideOpt_4090/amp/iupred2a/iupred2a.py -a /geniusland/home/liuxianliang1/code/PeptideOpt_4090/features/{i+1}.fasta short\")\n",
    "    res = result.read()\n",
    "    short_val = res.splitlines()[0].split(',')\n",
    "    short_list = np.zeros((len(short_val), 1))\n",
    "    short_list[0][0] = float(short_val[0].split('[')[1])\n",
    "    for j in range(1, len(short_val)-1):\n",
    "        short_list[j][0] = float(short_val[j])\n",
    "    short_list[j][0] = float(short_val[j].split(']')[0])\n",
    "\n",
    "    anchor_val = res.splitlines()[1].split(',')\n",
    "    anchor_list = np.zeros((len(anchor_val), 1))\n",
    "    anchor_list[0][0] = float(anchor_val[0].split('[')[1])\n",
    "    for j in range(1, len(anchor_val)-1):\n",
    "        anchor_list[j][0] = float(anchor_val[j])\n",
    "    anchor_list[j][0] = float(anchor_val[j].split(']')[0])\n",
    "\n",
    "    results = np.concatenate((long_list, short_list, anchor_list), axis=1)\n",
    "    pep_intrinsic_dict[peptides[i]] = results\n",
    "# 靶点蛋白质\n",
    "result = os.popen(\"python3 /geniusland/home/liuxianliang1/code/PeptideOpt_4090/amp/iupred2a/iupred2a.py -a /geniusland/home/liuxianliang1/code/PeptideOpt_4090/features/target_protein.fasta long\")\n",
    "res = result.read()\n",
    "long_val = res.splitlines()[0].split(',')\n",
    "long_list = np.zeros((len(long_val), 1))\n",
    "long_list[0][0] = float(long_val[0].split('[')[1])\n",
    "for j in range(1, len(long_val)-1):\n",
    "    long_list[j][0] = float(long_val[j])\n",
    "long_list[j][0] = float(long_val[j].split(']')[0])\n",
    "\n",
    "result = os.popen(\"python3 /geniusland/home/liuxianliang1/code/PeptideOpt_4090/amp/iupred2a/iupred2a.py -a /geniusland/home/liuxianliang1/code/PeptideOpt_4090/features/target_protein.fasta short\")\n",
    "res = result.read()\n",
    "short_val = res.splitlines()[0].split(',')\n",
    "short_list = np.zeros((len(short_val), 1))\n",
    "short_list[0][0] = float(short_val[0].split('[')[1])\n",
    "for j in range(1, len(short_val)-1):\n",
    "    short_list[j][0] = float(short_val[j])\n",
    "short_list[j][0] = float(short_val[j].split(']')[0])\n",
    "\n",
    "anchor_val = res.splitlines()[1].split(',')\n",
    "anchor_list = np.zeros((len(anchor_val), 1))\n",
    "anchor_list[0][0] = float(anchor_val[0].split('[')[1])\n",
    "for j in range(1, len(anchor_val)-1):\n",
    "    anchor_list[j][0] = float(anchor_val[j])\n",
    "anchor_list[j][0] = float(anchor_val[j].split(']')[0])\n",
    "results = np.concatenate((long_list, short_list, anchor_list), axis=1)\n",
    "prot_intrinsic_dict[protein] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load feature dict\n"
     ]
    }
   ],
   "source": [
    "protein_dense_feature_dict = np.concatenate((prot_pssm_dict[protein], prot_intrinsic_dict[protein]),axis=1)\n",
    "with open('features/protein_dense_feature_dict','wb') as f:\n",
    "    pickle.dump(protein_dense_feature_dict,f)\n",
    "\n",
    "f = open('features/example.tsv')\n",
    "pep_set = set()\n",
    "seq_set = set()\n",
    "pep_ss_set = set()\n",
    "seq_ss_set = set()\n",
    "for line in f.readlines()[1:]: # if the file has headers and pay attention to the columns (whether have peptide binding site labels)\n",
    "    # seq, pep, label, pep_ss, seq_ss  = line.strip().split('\\t')\n",
    "    seq, pep, pep_ss, seq_ss  = line.strip().split('\\t')\n",
    "    pep_set.add(pep)\n",
    "    seq_set.add(seq)\n",
    "    pep_ss_set.add(pep_ss)\n",
    "    seq_ss_set.add(seq_ss)\n",
    "\n",
    "f.close()\n",
    "pep_len = [len(pep) for pep in pep_set]\n",
    "seq_len = [len(seq) for seq in seq_set]\n",
    "pep_ss_len = [len(pep_ss) for pep_ss in pep_ss_set]\n",
    "seq_ss_len = [len(seq_ss) for seq_ss in seq_ss_set]\n",
    "\n",
    "pep_len.sort()\n",
    "seq_len.sort()\n",
    "pep_ss_len.sort()\n",
    "seq_ss_len.sort()\n",
    "pad_pep_len = 50 \n",
    "pad_prot_len = seq_len[int(0.8*len(seq_len))-1]\n",
    "# print('num of peptides', len(pep_len), 'pad_pep_len', pad_pep_len)\n",
    "# print('seq_set', len(seq_len), 'pad_prot_len', pad_prot_len)\n",
    "# print('num of peptide ss', len(pep_ss_len), 'pad_pep_len', pad_pep_len)\n",
    "# print('seq_ss_set', len(seq_ss_len), 'pad_prot_len', pad_prot_len)\n",
    "# np.save('features/pad_pep_len',pad_pep_len)\n",
    "# np.save('features/pad_prot_len',pad_prot_len)\n",
    "\n",
    "peptide_feature_dict = {}\n",
    "protein_feature_dict = {}\n",
    "\n",
    "peptide_ss_feature_dict = {}\n",
    "protein_ss_feature_dict = {}\n",
    "\n",
    "peptide_2_feature_dict = {}\n",
    "protein_2_feature_dict = {}\n",
    "\n",
    "peptide_dense_feature_dict = {}\n",
    "protein_dense_feature_dict = {}\n",
    "\n",
    "f = open('features/example.tsv')\n",
    "for line in f.readlines()[1:]:\n",
    "    seq, pep, pep_ss, seq_ss  = line.strip().split('\\t')\n",
    "    if pep not in peptide_feature_dict:\n",
    "        feature = label_sequence(pep, pad_pep_len, amino_acid_set)\n",
    "        peptide_feature_dict[pep] = feature\n",
    "    if seq not in protein_feature_dict:\n",
    "        feature = label_sequence(seq, pad_prot_len, amino_acid_set)\n",
    "        protein_feature_dict[seq] = feature\n",
    "    if pep_ss not in peptide_ss_feature_dict:\n",
    "        feature = label_seq_ss(pep_ss, pad_pep_len, seq_ss_dict)\n",
    "        peptide_ss_feature_dict[pep_ss] = feature\n",
    "    if seq_ss not in protein_ss_feature_dict:\n",
    "        feature = label_seq_ss(seq_ss, pad_prot_len, seq_ss_dict)\n",
    "        protein_ss_feature_dict[seq_ss] = feature\n",
    "    if pep not in peptide_2_feature_dict:\n",
    "        feature = label_sequence(pep, pad_pep_len, physicochemical_set)\n",
    "        peptide_2_feature_dict[pep] = feature\n",
    "    if seq not in protein_2_feature_dict:\n",
    "        feature = label_sequence(seq, pad_prot_len, physicochemical_set)\n",
    "        protein_2_feature_dict[seq] = feature\n",
    "    if pep not in peptide_dense_feature_dict:\n",
    "        feature = padding_intrinsic_disorder(pep_intrinsic_dict[pep], pad_pep_len)\n",
    "        peptide_dense_feature_dict[pep] = feature\n",
    "    if seq not in protein_dense_feature_dict:\n",
    "        feature_pssm = padding_sigmoid_pssm(prot_pssm_dict[seq], pad_prot_len)\n",
    "        feature_intrinsic = padding_intrinsic_disorder(prot_intrinsic_dict[seq], pad_prot_len)\n",
    "        feature_dense = np.concatenate((feature_pssm, feature_intrinsic), axis=1)\n",
    "        protein_dense_feature_dict[seq] = feature_dense\n",
    "\n",
    "f.close()\n",
    "\n",
    "print('load feature dict')\n",
    "X_pep, X_prot, X_pep_SS, X_prot_SS, X_pep_2, X_prot_2 = [], [], [], [], [], []\n",
    "X_dense_pep,X_dense_prot = [],[]\n",
    "pep_sequence, prot_sequence, Y = [], [], []\n",
    "with open('features/example.tsv') as f:  # change your own data here\n",
    "    for line in f.readlines()[1:]:\n",
    "        protein, peptide, pep_ss, prot_ss  = line.strip().split('\\t')\n",
    "        # protein, peptide,label, pep_ss, prot_ss  = line.strip().split('\\t')\n",
    "        pep_sequence.append(peptide)\n",
    "        prot_sequence.append(protein)\n",
    "\n",
    "        X_pep.append(peptide_feature_dict[peptide])\n",
    "        X_prot.append(protein_feature_dict[protein])\n",
    "        X_pep_SS.append(peptide_ss_feature_dict[pep_ss])\n",
    "        X_prot_SS.append(protein_ss_feature_dict[prot_ss])\n",
    "        X_pep_2.append(peptide_2_feature_dict[peptide])\n",
    "        X_prot_2.append(protein_2_feature_dict[protein])\n",
    "        X_dense_pep.append(peptide_dense_feature_dict[peptide])\n",
    "        X_dense_prot.append(protein_dense_feature_dict[protein])\n",
    "        \n",
    "X_pep = torch.from_numpy(np.array(X_pep)).to(device)\n",
    "X_prot = torch.from_numpy(np.array(X_prot)).to(device)\n",
    "X_pep_ss = torch.from_numpy(np.array(X_pep_SS)).to(device)\n",
    "X_prot_ss = torch.from_numpy(np.array(X_prot_SS)).to(device)\n",
    "X_pep_2 = torch.from_numpy(np.array(X_pep_2)).to(device)\n",
    "X_prot_2 = torch.from_numpy(np.array(X_prot_2)).to(device)\n",
    "X_pep_dense = torch.from_numpy(np.array(X_dense_pep)).to(device)\n",
    "X_prot_dense = torch.from_numpy(np.array(X_dense_prot)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "camp.eval()\n",
    "preds = []\n",
    "pred=camp(X_pep,X_prot,X_pep_ss,X_prot_ss,X_pep_2,X_prot_2,X_pep_dense,X_prot_dense)\n",
    "preds.extend(pred.detach().cpu().numpy().tolist())\n",
    "preds = np.array(preds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09502498805522919\n",
      "0.027402497828006744\n",
      "0.014363469555974007\n",
      "0.0008419280056841671\n",
      "3.308707891847007e-05\n",
      "0.006236928049474955\n",
      "5.825800286629601e-08\n",
      "3.649595382848592e-12\n",
      "6.020334097911473e-08\n",
      "0.0864051803946495\n"
     ]
    }
   ],
   "source": [
    "for i in preds:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009264955879189074\n",
      "0.01075808983296156\n",
      "3.281701594914921e-07\n",
      "0.007561501115560532\n",
      "0.2916243076324463\n",
      "5.587330451817252e-05\n",
      "4.477945481085044e-07\n",
      "2.7672586444538183e-10\n",
      "0.00021700849174521863\n",
      "0.0028090523555874825\n",
      "0.0014142475556582212\n"
     ]
    }
   ],
   "source": [
    "for i in preds:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06463415920734406\n",
      "0.9980089068412781\n",
      "0.03452111780643463\n",
      "0.9999321699142456\n",
      "0.0005423002294264734\n",
      "8.511067335348343e-07\n",
      "5.082506030623657e-12\n",
      "3.1918244047801636e-08\n",
      "6.124983853439403e-11\n",
      "0.30908203125\n",
      "2.2646936486125924e-06\n"
     ]
    }
   ],
   "source": [
    "for i in preds:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('hydramp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78ab29468e18f64eb9e96ed2e1dab4bddb88cf91b798c80e4abb022d4f2a601c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
